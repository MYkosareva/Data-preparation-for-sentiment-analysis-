Описание задачи:
Имеется текстовый файл со списком статей. Каждая новая статья начинается с новой строки и представляет собой словарь,
содержащий 3 значения:
- url
- title
- article

Задача:
Нужно сделать sentiment analysis для каждой из статей, предварительно подготовив каждую статью к анализу.

Подготовка включает в себя:
- чистка данных, без потери самого текста статьи
- нормализация данных
- обработка данных

Инструменты, которыми можно (и нужно) пользоваться (на выбор или один из):
- spacy
- nltk

Ожидаемый результат:
Имеется JSON-файл, содержащий в себе словарь с теми же данными, которые есть в изначальном корпусе, НО текст должен быть
очищен от сторонних символов и добавлена эмоциональная оценка текста (лейблы можно использовать, будет плюсом,
но можно использовать и только числовые значения).

Вопросы:
1) Какой метод нормализации был выбран? Почему?
2) Какая метрика используется для sentiment analysis?
3) С какими сложностями столкнулась во время работы?

Срок выполнения:
Нужно подготовить это задание и отправить на проверку до утра пятницы, 15го мая.
Правила оформления:
Готовый к проверке код нужно закоммитить на github.com и отправить ссылку Саше.
Нужно убедиться, что репозиторий не приватен и к нему имеется доступ.
Код должен быть закомментирован (docstring).
Будет плюсом, если это будет выполнено в виде интерактивного API, в качестве сервера можно указать локалхост, чтобы
можно было запустить сервис на своей машине, а еще лучше - если все будет докеризировано, но это опционально.

Удачи!